{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8vQV1_8Bjf5",
        "outputId": "8b2e6807-c264-405e-bfae-1b8b4be7be4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file exists, skipping\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"train.csv\"):\n",
        "   from datasets import *\n",
        "   dset = load_dataset(\"dair-ai/emotion\")\n",
        "   # code from https://stackoverflow.com/a/76218276\n",
        "   train_testvalid = dset['train'].train_test_split(test_size=0.2)\n",
        "   # Split the 10% test + valid in half test, half valid\n",
        "   test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "   # gather everyone if you want to have a single DatasetDict\n",
        "   dset = DatasetDict({\n",
        "      'train': train_testvalid['train'],\n",
        "      'test': test_valid['test'],\n",
        "      'valid': test_valid['train']})\n",
        "   dset   \n",
        "else:\n",
        "    print(\"file exists, skipping\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzeN4ah3-sOZ"
      },
      "source": [
        "## Cleaning Function to clean the dataset text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "g0I8C9j19f6s",
        "outputId": "f9cd56f8-4f91-4907-cd63-9a1eee97fdc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file exists, skipping\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "if not os.path.exists(\"test.csv\"):\n",
        "    # Check if the spaCy model is loaded, otherwise install it\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"Downloading the 'en_core_web_sm' model\")\n",
        "        !python -m spacy download en_core_web_sm\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    train_df = pd.DataFrame(dset[\"train\"])\n",
        "    test_df = pd.DataFrame(dset['test'])\n",
        "    val_df = pd.DataFrame(dset['validation'])\n",
        "    i = 0\n",
        "    def cleaning(text):\n",
        "        doc = nlp(text.lower().strip())\n",
        "        cleaned_tokens = []\n",
        "        global i\n",
        "        print(f\"Cleaning: {i}\")\n",
        "        for token in doc:\n",
        "            if not token.is_stop and not token.is_punct and not token.is_space:\n",
        "                lemma = re.sub(r'\\W', '', token.lemma_)\n",
        "                if lemma:\n",
        "                    cleaned_tokens.append(lemma)\n",
        "        i += 1\n",
        "        return ' '.join(cleaned_tokens)\n",
        "\n",
        "    # Apply the cleaning function to the text column\n",
        "    train_df[\"cleaned_text\"] = train_df[\"text\"].apply(cleaning)\n",
        "    test_df[\"cleaned_text\"] = test_df[\"text\"].apply(cleaning)\n",
        "    val_df[\"cleaned_text\"] = val_df[\"text\"].apply(cleaning)\n",
        "\n",
        "    print(train_df[['text', 'cleaned_text']].head())  # Display the original and cleaned text for verification\n",
        "\n",
        "    train_df.to_csv(\"train.csv\")\n",
        "    test_df.to_csv(\"test.csv\")\n",
        "    val_df.to_csv(\"val.csv\")\n",
        "\n",
        "else: \n",
        "    print(\"file exists, skipping\")\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "    val_df = pd.read_csv(\"val.csv\")    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is to remove the 5 from the dfs\n",
        "train_df = train_df[train_df['label'] != 5]\n",
        "test_df = test_df[test_df['label'] != 5]\n",
        "val_df = val_df[val_df['label'] != 5]\n",
        "\n",
        "train_df = train_df.dropna(subset=[\"cleaned_text\"])\n",
        "test_df = test_df.dropna(subset=[\"cleaned_text\"])\n",
        "val_df = val_df.dropna(subset=[\"cleaned_text\"])\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4 Different types of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "TK1J3A5wJl1u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words features: (15428, 11592)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X_bow = vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_bow = vectorizer.transform(test_df['cleaned_text'])\n",
        "print(\"Bag of Words features:\", X_bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "V-pQAWi8JmEi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF features: (15428, 11592)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "X_test_tfidf = vectorizer.transform(test_df['cleaned_text'])\n",
        "print(\"TF-IDF features:\", X_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qw0END3qJmTo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec features shape: (15428, 100)\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "def create_word2vec_embeddings(dataframe):\n",
        "    sentences = [text.split() for text in dataframe['cleaned_text']]\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    word_vectors = model.wv\n",
        "\n",
        "    # Average Word Vectors for each text\n",
        "    def document_vector(doc):\n",
        "        return np.mean([word_vectors[w] for w in doc if w in word_vectors], axis=0)\n",
        "\n",
        "    X_w2v = np.array([document_vector(text) for text in sentences if document_vector(text).shape != ()])\n",
        "    return X_w2v\n",
        "\n",
        "\n",
        "X_w2v = create_word2vec_embeddings(train_df)\n",
        "X_test_w2v = create_word2vec_embeddings(test_df)\n",
        "print(\"Word2Vec features shape:\", X_w2v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "SqA9Kgz7KPTq"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91cf3ebd72e948e9a116df162151ace8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/483 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8289824ba67c4fc9a17de9329e99fdd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT Embeddings shape: (15428, 384)\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "#if not os.path.exists('X_bert.npy'):\n",
        "def create_bert_embeddings(dataframe):\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    X_bert = model.encode(dataframe['cleaned_text'], show_progress_bar=True)\n",
        "    return X_bert\n",
        "\n",
        "X_bert = create_bert_embeddings(train_df)\n",
        "X_test_bert = create_bert_embeddings(test_df)\n",
        "np.save('X_bert.npy', X_bert)\n",
        "np.save('X_test_bert.npy', X_test_bert)\n",
        "\n",
        "#else:    \n",
        "#    print(\"file exists, skipping\")\n",
        "#    X_bert = np.load('X_bert.npy')\n",
        "#    X_test_bert = np.load('X_test_bert.npy') \n",
        "\n",
        "print(\"BERT Embeddings shape:\", X_bert.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0                                                   7419\n",
              "text            i was feeling a bit like the internet is repla...\n",
              "label                                                           1\n",
              "cleaned_text    feel bit like internet replace valuable face f...\n",
              "Name: 7122, dtype: object"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.loc[7122]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "O7L5DQD-9f6u"
      },
      "outputs": [],
      "source": [
        "y_train = (train_df['label'])\n",
        "y_test = (test_df['label'])\n",
        "y_val = val_df['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "wL7a4pix9f6x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## W2V Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Za8u9TwvHCJr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.74      0.65       581\n",
            "           1       0.60      0.87      0.71       695\n",
            "           2       0.61      0.07      0.12       159\n",
            "           3       0.77      0.25      0.37       275\n",
            "           4       0.85      0.31      0.46       224\n",
            "\n",
            "    accuracy                           0.61      1934\n",
            "   macro avg       0.68      0.45      0.46      1934\n",
            "weighted avg       0.65      0.61      0.57      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, random_state=82)\n",
        "clf.fit(X_bert, y_train)\n",
        "y_pred_test = clf.predict(X_test_bert)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.74      0.73       581\n",
            "           1       0.75      0.79      0.77       695\n",
            "           2       0.53      0.45      0.49       159\n",
            "           3       0.64      0.57      0.60       275\n",
            "           4       0.66      0.69      0.68       224\n",
            "\n",
            "    accuracy                           0.70      1934\n",
            "   macro avg       0.66      0.65      0.65      1934\n",
            "weighted avg       0.70      0.70      0.70      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf2 = LogisticRegression(penalty=None, max_iter=5000)\n",
        "clf2.fit(X_bert, y_train)\n",
        "y_pred_test2 = clf2.predict(X_test_bert)\n",
        "print(classification_report(y_test, y_pred_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BoW Encoding for RFC & Log. Reg.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       581\n",
            "           1       0.90      0.90      0.90       695\n",
            "           2       0.71      0.72      0.72       159\n",
            "           3       0.86      0.83      0.85       275\n",
            "           4       0.89      0.89      0.89       224\n",
            "\n",
            "    accuracy                           0.88      1934\n",
            "   macro avg       0.85      0.85      0.85      1934\n",
            "weighted avg       0.88      0.88      0.88      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf3 = RandomForestClassifier(n_estimators=100, random_state=82)\n",
        "clf3.fit(X_bow, y_train)\n",
        "y_pred_test = clf3.predict(X_test_bow)\n",
        "print(classification_report(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88       581\n",
            "           1       0.90      0.89      0.89       695\n",
            "           2       0.64      0.69      0.67       159\n",
            "           3       0.83      0.80      0.82       275\n",
            "           4       0.82      0.87      0.84       224\n",
            "\n",
            "    accuracy                           0.85      1934\n",
            "   macro avg       0.82      0.82      0.82      1934\n",
            "weighted avg       0.85      0.85      0.85      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf2 = LogisticRegression(penalty=None, max_iter=5000)\n",
        "clf2.fit(X_bow, y_train)\n",
        "y_pred_test2 = clf2.predict(X_test_bow)\n",
        "print(classification_report(y_test, y_pred_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.91       581\n",
            "           1       0.87      0.93      0.90       695\n",
            "           2       0.80      0.66      0.72       159\n",
            "           3       0.89      0.84      0.86       275\n",
            "           4       0.90      0.88      0.89       224\n",
            "\n",
            "    accuracy                           0.88      1934\n",
            "   macro avg       0.87      0.84      0.86      1934\n",
            "weighted avg       0.88      0.88      0.88      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, random_state=82)\n",
        "clf.fit(X_tfidf, y_train)\n",
        "y_pred_test = clf.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90       581\n",
            "           1       0.90      0.90      0.90       695\n",
            "           2       0.67      0.70      0.69       159\n",
            "           3       0.86      0.83      0.84       275\n",
            "           4       0.85      0.85      0.85       224\n",
            "\n",
            "    accuracy                           0.87      1934\n",
            "   macro avg       0.83      0.83      0.83      1934\n",
            "weighted avg       0.87      0.87      0.87      1934\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf2 = LogisticRegression(penalty=None, max_iter=5000)\n",
        "clf2.fit(X_tfidf, y_train)\n",
        "y_pred_test2 = clf2.predict(X_test_tfidf)\n",
        "print(classification_report(y_test, y_pred_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Encoding for RFC & Log. Reg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.19      0.24       581\n",
            "           1       0.27      0.02      0.04       695\n",
            "           2       0.00      0.00      0.00       159\n",
            "           3       0.15      0.30      0.20       275\n",
            "           4       0.11      0.50      0.18       224\n",
            "\n",
            "    accuracy                           0.16      1934\n",
            "   macro avg       0.17      0.20      0.13      1934\n",
            "weighted avg       0.23      0.16      0.14      1934\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, random_state=82)\n",
        "clf.fit(X_w2v, y_train)\n",
        "y_pred_test = clf.predict(X_test_w2v)\n",
        "print(classification_report(y_test, y_pred_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       581\n",
            "           1       1.00      0.00      0.00       695\n",
            "           2       0.00      0.00      0.00       159\n",
            "           3       0.14      1.00      0.25       275\n",
            "           4       0.00      0.00      0.00       224\n",
            "\n",
            "    accuracy                           0.14      1934\n",
            "   macro avg       0.23      0.20      0.05      1934\n",
            "weighted avg       0.38      0.14      0.04      1934\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "clf2 = LogisticRegression(penalty=None, max_iter=5000)\n",
        "clf2.fit(X_w2v, y_train)\n",
        "y_pred_test2 = clf2.predict(X_test_w2v)\n",
        "print(classification_report(y_test, y_pred_test2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "dc2d302daea6488b9f391329acad855e",
    "deepnote_persisted_session": {
      "createdAt": "2024-04-16T05:00:32.397Z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
